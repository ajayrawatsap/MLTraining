{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Dataset\n",
    "http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "MNIST (\"Modified National Institute of Standards and Technology\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\n",
    "The MNIST database contains 60,000 training images and 10,000 testing images.\n",
    "![title](mnist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.display.max_columns = None\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data \n",
    "The MNIST data comes pre-loaded with sklearn. The first 60000 images are training data and next 1000 are test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "X, y = mnist['data'], mnist['target']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "\n",
    "print('Training Shape {} Test Shape {}'.format(X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Validation set\n",
    "In real world ML scenarios we create separate Train, Validation and Test set. We train our model on Training set,  optimize our model using validation set and evalaute on Test set so that we dont induce bias.  Since we alreday have test set we need to split training set into separate traiining and validation sets. As we will see later that we can do K-fold cross valaidtion which removes the necessaity of creating Validations sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.2, \n",
    "                                                     random_state = random_state, stratify=  y_train )\n",
    "print('Training Shape {} Validation Shape {}'.format(X_train.shape, X_valid.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Display Sample Image\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "def display_digit(digit):\n",
    "# digit = X_train[2]\n",
    "    digit_image = digit.reshape(28,28)\n",
    "    plt.imshow(digit_image, cmap = matplotlib.cm.binary, interpolation = 'nearest')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit = X_train[2]\n",
    "display_digit(digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Each Image consist of 28 X 28 pixels with pixel values from 0 to 255. The pixel values represent the greyscale intensity increasing from 0 to 255. As we can see below digit 6 can be represented by pixel intensities of varying values and the region where pixel intensities has high value are assosciated with the image of 6\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(digit.reshape(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Traget Value Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(y_train)[0].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model Using Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model  = DecisionTreeClassifier(random_state = random_state)\n",
    "model.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Set Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = model.predict(X_valid)\n",
    "test_acc = accuracy_score(y_valid, y_pred)\n",
    "print('Validation accuracy', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model Using Random Forest:Default "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(random_state = random_state, n_jobs = 4)\n",
    "model.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation set accuracy\n",
    "We can see that just using default parameters, we are able to achieve better accuracy on Random Forest compared to a single Decision tree. Random Forest by default uses 10 decision trees to make predcitions and the end results is combined prediction of all trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_valid)\n",
    "test_acc = accuracy_score(y_valid, y_pred)\n",
    "print('Validation accuracy', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model Using Random Forest: Tuned HyperParameters\n",
    "The Hyperparameters were manually Tuned, we will later see serach algorithms to find best hyperparameters automatically\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(  n_estimators = 80,\n",
    "                                  criterion = 'entropy',\n",
    "                                  bootstrap =  False,\n",
    "                                  max_depth   =  30,\n",
    "                                 verbose = 1,\n",
    "                                 random_state = random_state,\n",
    "                                 n_jobs = 4\n",
    "                                 )\n",
    "model.fit(X_train, y_train) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Set Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_valid)\n",
    "test_acc = accuracy_score(y_valid, y_pred)\n",
    "print('Validation accuracy', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Set Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "print('Validation accuracy', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Incorrect Predictions\n",
    "Lets display random 10 images in test data which were incorrectly predicted by our model.\n",
    "We can notice some of the images are difficult to identify even for humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def display_incorrect_preds(y_test, y_pred):\n",
    "    test_labels = pd.DataFrame()\n",
    "    test_labels['actual'] = y_test\n",
    "    test_labels['pred'] = y_pred\n",
    "    incorrect_pred = test_labels[test_labels['actual'] != test_labels['pred'] ]\n",
    "    random_incorrect_pred = incorrect_pred.sample(n= 10)\n",
    "\n",
    "    for i, row in random_incorrect_pred.iterrows():\n",
    "        print('Actual Value:', row['actual'], 'Predicted Value:', row['pred'])\n",
    "        display_digit(X_test[i])\n",
    "        \n",
    " \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_incorrect_preds(y_test, y_pred)      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
