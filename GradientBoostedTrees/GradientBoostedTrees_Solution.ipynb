{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Dataset\n",
    "http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "MNIST (\"Modified National Institute of Standards and Technology\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\n",
    "The MNIST database contains 60,000 training images and 10,000 testing images.\n",
    "![title](mnist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.display.max_columns = None\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def timer_start():\n",
    "    global t0\n",
    "    t0 = time.time()\n",
    "\n",
    "\n",
    "def timer_end():\n",
    "    t1 = time.time()   \n",
    "    total = t1-t0\n",
    "    print('Time elapsed', total) \n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data \n",
    "The MNIST data comes pre-loaded with sklearn. The first 60000 images are training data and next 1000 are test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "X, y = mnist['data'], mnist['target']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "\n",
    "print('Training Shape {} Test Shape {}'.format(X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Validation set\n",
    "In real world ML scenarios we create separate Train, Validation and Test set. We train our model on Training set,  optimize our model using validation set and evalaute on Test set so that we dont induce bias.  Since we already have test set we need to split training set into separate traiining and validation sets. As we will see later that we can do K-fold cross validation which removes the necessaity of creating Validations set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.2, \n",
    "                                                     random_state = random_state, stratify=  y_train )\n",
    "print('Training Shape {} Validation Shape {}'.format(X_train.shape, X_valid.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Display Sample Image\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "def display_digit(digit):\n",
    "    digit_image = digit.reshape(28,28)\n",
    "    plt.imshow(digit_image, cmap = matplotlib.cm.binary, interpolation = 'nearest')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "   \n",
    "    \n",
    "digit = X_train[92]\n",
    "display_digit(digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Each Image consist of 28 X 28 pixels with pixel values from 0 to 255. The pixel values represent the greyscale intensity increasing from 0 to 255. As we can see below digit 4 can be represented by pixel intensities of varying values and the region where pixel intensities has high value are assosciated with the image of 4\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(digit.reshape(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Traget Value Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(y_train)[0].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model Using Gradient Boosted Machine\n",
    "The training on GBM is extremely slow for dataset this large. It is not feasable to use this for practical purpose hence code is commented. We will use a better alogorithm for boosted trees. For small dataset this still can be used hence code is not deleted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# timer_start()\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# model  = GradientBoostingClassifier(random_state = random_state,\n",
    "#                                     verbose = 1)\n",
    "# model.fit(X_train, y_train) \n",
    "# timer_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Set Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "# y_pred = model.predict(X_valid)\n",
    "# test_acc = accuracy_score(y_valid, y_pred)\n",
    "# print('Validation accuracy', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model Using LightGBM: Default\n",
    "\n",
    "LightGBM devloped by Microsoft Research Team, is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n",
    "\n",
    "<br>Faster training speed and higher efficiency\n",
    "<br>Lower memory usage\n",
    "<br>Better accuracy\n",
    "<br>Parallel and GPU learning supported\n",
    "<br>Capable of handling large-scale data\n",
    "\n",
    "https://lightgbm.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "timer_start()\n",
    "model = LGBMClassifier(  n_jobs = 4,                      \n",
    "                        random_state  = random_state ,\n",
    "                        objective =  'multiclass',\n",
    "                        num_class = 10  )\n",
    "model.fit(X_train, y_train )\n",
    "          \n",
    "timer_end()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation set accuracy\n",
    "The defualt  model gave an impressive accuracy of 97% comapared to 94.5% accuracy of RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = model.predict(X_valid)\n",
    "test_acc = accuracy_score(y_valid, y_pred)\n",
    "print('Validation accuracy', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model Using LightGBM:Tuned with Early Stopping\n",
    "The Idea behind early stopping is that we train the model for large number of iterations, but stop when the validation score stops improving. This is a powerful mechanism to deal with overfiiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer_start()\n",
    "model = LGBMClassifier(  n_jobs = 4,    \n",
    "                         n_estimators=10000,\n",
    "                         random_state  = random_state ,\n",
    "                         learning_rate = 0.1,\n",
    "                         subsample = 0.8,\n",
    "                         colsample_bytree= 0.8,\n",
    "                         subsample_freq = 1,                 \n",
    "                         objective =  'multiclass',\n",
    "                         num_class = 10  )\n",
    "model.fit( X_train, \n",
    "           y_train,\n",
    "           eval_set=[ (X_valid, y_valid)],\n",
    "           eval_metric= 'multi_error',\n",
    "           verbose= 40,\n",
    "           early_stopping_rounds= 50,      \n",
    "           )\n",
    "          \n",
    "timer_end()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Set Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_valid)\n",
    "test_acc = accuracy_score(y_valid, y_pred)\n",
    "print('Validation accuracy', test_acc, 'error', 1-test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Set Accuracy\n",
    "The Test Accuracy 98.21% of a tuned LightGBM Model is better than 97.06% of Tuned RandomForest. The increase of 1.2% may not seem much but it means  120 more correct predcition on  test set of 10000 samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "print('Validation accuracy', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Incorrect Predictions\n",
    "Lets display random 10 images in test data which were incorrectly predicted by our model. We can notice some of the images are difficult to identify even for humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def display_incorrect_preds(y_test, y_pred):\n",
    "    test_labels = pd.DataFrame()\n",
    "    test_labels['actual'] = y_test\n",
    "    test_labels['pred'] = y_pred\n",
    "    incorrect_pred = test_labels[test_labels['actual'] != test_labels['pred'] ]\n",
    "    random_incorrect_pred = incorrect_pred.sample(n= 10)\n",
    "\n",
    "    for i, row in random_incorrect_pred.iterrows():\n",
    "        print('Actual Value:', row['actual'], 'Predicted Value:', row['pred'])\n",
    "        display_digit(X_test[i])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_incorrect_preds(y_test, y_pred)        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
