{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IRIS Dataset\n",
    "Picture and text published from http://suruchifialoke.com/2016-10-13-machine-learning-tutorial-iris-classification/\n",
    "![title](iris.png)\n",
    "\n",
    "Three Iris varieties were used in the Iris flower data set outlined by Ronald Fisher in his famous 1936 paper “The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis” PDF. Since then his paper has been cited over 2000 times and the data set has been used by almost every data science beginner.\n",
    "\n",
    "The data set consists of:\n",
    "\n",
    "150 samples\n",
    "3 labels: species of Iris (Iris setosa, Iris virginica and Iris versicolor)\n",
    "\n",
    "4 features: length and the width of the sepals and petals, in centimetres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "random_state = 42\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn libraray comes with IRIS data pre-loaded, which we would convert into a pandas dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "def get_iris_dataset():\n",
    "    iris = datasets.load_iris()\n",
    "    iris_df = pd.DataFrame(iris.data, columns= iris.feature_names)\n",
    "    iris_df['target'] = iris.target\n",
    "    print('shape', iris_df.shape)\n",
    "    iris_df.head( )\n",
    "    return iris_df, iris.target_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target field in data represents the  species of flower which are mapped as mapped as \n",
    "<br>setosa : 0, \n",
    "<br>versicolor: 1 \n",
    "<br>virginica: 2.\n",
    "\n",
    "We can see  that there are 50  samples of each species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for Training\n",
    "We will only use two features  petal length  and petal width from the data so that it can be plotted on the graph. Also we will use  all three classes of flowers . This will be a multiclass classification problem to solve as we are predicted more than 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data into  training and test set\n",
    "Split the data into training set and test set such that number of samples in training and test set are in 80:20 ratio. The model will be trained on training set and then evalauted on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training Data\n",
    "There are three classes of flower, each class is represented by different color and plotted for two of the chosen features on x and y axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_classes(X_train, y_train, columns):\n",
    "    train_df = X_train.copy()\n",
    "    train_df['target'] = y_train\n",
    "    plt.figure(figsize = (10, 6))\n",
    "#     ax = sns.scatterplot(x= columns[0], y= columns[1], hue=\"target\", \n",
    "#                          style ='target', palette=\"Set1\", data= train_df)\n",
    "    train_0 = train_df[train_df['target'] == 0]\n",
    "    train_1 = train_df[train_df['target'] == 1]\n",
    "    train_2 = train_df[train_df['target'] == 2]\n",
    "    plt.scatter(x =train_0[columns[0]], y=  train_0[columns[1]], label = '0_setosa' )\n",
    "    plt.scatter(x =train_1[columns[0]], y=  train_1[columns[1]], label = '1_versicolor' )\n",
    "    plt.scatter(x =train_2[columns[0]], y=  train_2[columns[1]], label = '2_virginica' )\n",
    "    plt.legend( )\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model \n",
    "Train the model using traning set data and calling fit method\n",
    "\n",
    "The model documentaion can be accessed using link \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the tree splits\n",
    "The decison to split based on a fearure can be visualized in this graph. The feature is chosen, which gives best split and is measured by gini. The lower the gini score better the classification. A gini score of 0 means perfect calssification meaning all samples belong to a single class. The decsion trees grow their nodes to achive minimum gini score after each split. All features are scanned to slect the feature which gives best split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image  \n",
    "from sklearn.externals.six import StringIO  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "def get_tree_graph(clf):\n",
    "    dot_data = StringIO()\n",
    "    export_graphviz(clf, out_file=dot_data,  \n",
    "                    filled=True, rounded=True,\n",
    "                    special_characters=True)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "    return graph\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Tree Decision Boundaries on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_train_decision_boundary(X_train, y_train, model):\n",
    "    n_classes = 3\n",
    "    plot_colors = \"ryb\"\n",
    "    plot_step = 0.02\n",
    "\n",
    "    X = X_train.values\n",
    "    y = y_train.values\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
    "    \n",
    "    features = X_train.columns.tolist()\n",
    "\n",
    "    plt.xlabel(features[0])\n",
    "    plt.ylabel(features[1])\n",
    "\n",
    "    # # Plot the training points\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y == i)\n",
    "        plt.scatter(X[idx, 0], X[idx, 1], c=color,\n",
    "                    cmap=plt.cm.RdYlBu, edgecolor='black', s=15)\n",
    "\n",
    "    plt.show()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Tree Decision Boundaries on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_test_decision_boundary(X_train, y_train, X_test, y_test, clf):\n",
    "    n_classes = 3\n",
    "    plot_colors = \"ryb\"\n",
    "    plot_step = 0.02\n",
    "\n",
    "    X = X_train.values\n",
    "    y = y_train.values\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
    "    \n",
    "    features = X_train.columns.tolist()\n",
    "\n",
    "    plt.xlabel(features[0])\n",
    "    plt.ylabel(features[1])\n",
    "\n",
    "    # # Plot the test points\n",
    "    X = X_test.values\n",
    "    y = y_test.values\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y == i)\n",
    "        plt.scatter(X[idx, 0], X[idx, 1], c=color,# label= features[i],\n",
    "                    cmap=plt.cm.RdYlBu, edgecolor='black', s=15)\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Set Prediction and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with New Hyperparameters\n",
    "Let's try to improve accuracy for the test set. The previous model had good training accuracy, but slightly worse accuracy on the test set. This means our model overfitted on training data. We will now train same model using different hyperparameters to reduce overfitting so that our model generalizes better on unseen data (Test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the tree splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Tree Decision Boundaries on training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy\n",
    "By limiting the freedom of the the model by reducing the depth to 3 we are able to get better accuracy. By defualt depth of the tree is unlimited as was the case in our previous model. You can achieve similar results by tweaking the other hyperparameters like min_samples_leaf or min_samples_split. Welcome to world of hyperparameters tuning which will play important role going forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Tree Decision Boundaries on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
